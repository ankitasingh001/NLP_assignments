{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BiLSTM_POS.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLurqfKa2Uo1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "bfb9396d-c282-4822-b19d-c331aefa6e3e"
      },
      "source": [
        "import nltk\n",
        "nltk.download('brown')\n",
        "nltk.download('universal_tagset')\n",
        "nltk.download('tagsets')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data] Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]   Unzipping help/tagsets.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pjz-pqaH2cdb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.corpus import brown #Brown Corpus \n",
        "from collections import defaultdict \n",
        "from sklearn.metrics import confusion_matrix,plot_confusion_matrix\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras import backend as K\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, InputLayer, Bidirectional, TimeDistributed, Embedding, Activation\n",
        "from keras.optimizers import Adam\n",
        "from nltk.metrics import ConfusionMatrix\n",
        "from itertools import chain "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7n1wvXx2in4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "Constant declarations\n",
        "'''\n",
        "UNIVERSAL_TAGSET =['NOUN', 'DET', 'ADJ', 'ADP', '.', 'VERB', 'CONJ', 'NUM', 'ADV', 'PRT', 'PRON', 'X']\n",
        "TOTAL_TAGGED_WORDS = len(brown.tagged_words())\n",
        "tagged_sentences = brown.tagged_sents(tagset = \"universal\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uU0ddrUVND94",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "Pre-processing\n",
        "'''\n",
        "\n",
        "def separate_tags(tagged_sentences):\n",
        "  ''' \n",
        "  Separating tags from the words in sentence\n",
        "  '''\n",
        "  sentences, sentence_tags = [], []\n",
        "  for sentence_plus_tag in tagged_sentences:\n",
        "    sentence, tags = zip(*sentence_plus_tag)\n",
        "    sentences.append(np.array(sentence))\n",
        "    sentence_tags.append(np.array(tags))\n",
        "  return sentences, sentence_tags\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NkzsZG44T9-T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BiLSTM_POS():\n",
        "\n",
        "  def __init__(self, train_sentences, test_sentences, train_tags, test_tags):\n",
        "    self.train_sentences = train_sentences\n",
        "    self.train_tags = train_tags\n",
        "    self.test_sentences = test_sentences\n",
        "    self.test_tags = test_tags\n",
        "    self.word_index = {}\n",
        "    self.tag_index = {}\n",
        "    self.train_dataX =[]\n",
        "    self.train_dataY = []\n",
        "    self.test_dataX = []\n",
        "    self.test_dataY = []\n",
        "    self.MAX_LENGTH=0\n",
        "    self.tag_metrics = defaultdict(lambda: defaultdict(lambda:0))\n",
        "    self.test_pred_dataY = []\n",
        "    self.model = Sequential()\n",
        "\n",
        "\n",
        "  def word_to_dict(self):\n",
        "    '''\n",
        "    Creating the dictionary to convert words (textual data) into integers for keras\n",
        "    '''\n",
        "\n",
        "    unique_words, unique_tags = set([]), set([])\n",
        "\n",
        "    self.word_index['PAD']=0\n",
        "    self.word_index['OOV']=1\n",
        "    self.tag_index['PAD']=0\n",
        "\n",
        "    for sent in train_sentences:\n",
        "      for w in sent:\n",
        "        unique_words.add(w.lower())\n",
        "\n",
        "    for tags in train_tags:\n",
        "      for tag in tags:\n",
        "        unique_tags.add(tag)\n",
        "\n",
        "    for i, w in enumerate(list(unique_words)):\n",
        "      self.word_index[w]=i+2\n",
        "\n",
        "    for i, t in enumerate(list(unique_tags)):\n",
        "      self.tag_index[t]=i+1\n",
        "\n",
        "  def word_to_vec(self):\n",
        "    '''\n",
        "    conversion of data to integer data\n",
        "    '''\n",
        "    for sent in self.train_sentences:\n",
        "      X =[]\n",
        "      for w in sent:\n",
        "        try:\n",
        "          X.append(self.word_index[w.lower()])\n",
        "        except KeyError:\n",
        "          X.append(self.word_index['OOV'])\n",
        "      self.train_dataX.append(X)\n",
        "\n",
        "    for sent in self.test_sentences:\n",
        "      X =[]\n",
        "      for w in sent:\n",
        "        try:\n",
        "          X.append(self.word_index[w.lower()])\n",
        "        except KeyError:\n",
        "          X.append(self.word_index['OOV'])\n",
        "      self.test_dataX.append(X)\n",
        "\n",
        "    for sent in self.train_tags:\n",
        "      X =[]\n",
        "      for t in sent:\n",
        "        X.append(self.tag_index[t])\n",
        "      self.train_dataY.append(X)\n",
        "\n",
        "    for sent in self.test_tags:\n",
        "      X =[]\n",
        "      for t in sent:\n",
        "        X.append(self.tag_index[t])\n",
        "      self.test_dataY.append(X)\n",
        "\n",
        "  \n",
        "  def max_length(self):\n",
        "    '''\n",
        "    maximum length of statement\n",
        "    '''\n",
        "    self.MAX_LENGTH = len(max(self.train_dataX, key=len))\n",
        "\n",
        "  def padding_sequences(self):\n",
        "    '''\n",
        "    Pad the data and make sentence of equal length\n",
        "    '''\n",
        "    self.train_dataX = pad_sequences(self.train_dataX, maxlen=self.MAX_LENGTH, padding='post')\n",
        "    self.test_dataX = pad_sequences(self.test_dataX, maxlen=self.MAX_LENGTH, padding='post')\n",
        "    self.train_dataY = pad_sequences(self.train_dataY, maxlen=self.MAX_LENGTH, padding='post')\n",
        "    self.test_dataY = pad_sequences(self.test_dataY, maxlen=self.MAX_LENGTH, padding='post')\n",
        "\n",
        "  def generate(self):\n",
        "    self.word_to_dict()\n",
        "    self.word_to_vec()\n",
        "    self.max_length()\n",
        "    self.padding_sequences()\n",
        "    self.Model()\n",
        "    self.model.fit(self.train_dataX, self.to_categorical(self.train_dataY, len(self.tag_index)), batch_size=128, epochs=4, validation_split=0.1)\n",
        "    self.test_pred_dataY = self.model.predict_classes(self.test_dataX)\n",
        "    self.calc_tag_metrics()\n",
        "\n",
        "  def conversion_to_tag(self):\n",
        "    '''\n",
        "    Conversion of Tags from integer to words (0->PAD)\n",
        "    '''\n",
        "    tag_indexes = {}\n",
        "    for i in self.tag_index:\n",
        "      j = self.tag_index[i]\n",
        "      tag_indexes[j]=i\n",
        "    tag_indexes[0] = 'PAD'\n",
        "    predicted_tags =[]\n",
        "    actual_tags = []\n",
        "\n",
        "    for i in range(len(self.test_pred_dataY)):\n",
        "      X=[]\n",
        "      for j in range(len(self.test_pred_dataY[i])):\n",
        "        X.append(tag_indexes[self.test_pred_dataY[i][j]])\n",
        "      predicted_tags.append(X)\n",
        "\n",
        "    for i in range(len(self.test_dataY)):\n",
        "      X= []\n",
        "      for j in range(len(self.test_dataY[i])):\n",
        "        X.append(tag_indexes[self.test_dataY[i]])\n",
        "      actual_tags.appedn(X)\n",
        "\n",
        "    self.test_pred_dataY = predicted_tags\n",
        "    self.test_dataY = actual_tags\n",
        "\n",
        "\n",
        "  \n",
        "  def Model(self):\n",
        "    '''\n",
        "    Bi_LSTM Model\n",
        "    '''\n",
        "    self.model = Sequential()\n",
        "    self.model.add(InputLayer(input_shape=(self.MAX_LENGTH, )))  ### Input Layer in BiLSTM\n",
        "    self.model.add(Embedding(len(self.word_index), 128))    #### Embedding LAyer to convert data to context vectors of len 128\n",
        "    self.model.add(Bidirectional(LSTM(256, return_sequences=True)))  #### BiLSTM with return sequence true\n",
        "    self.model.add(TimeDistributed(Dense(len(self.tag_index))))    #### Output Layer\n",
        "    self.model.add(Activation('softmax'))  #### activation Function\n",
        " \n",
        "    self.model.compile(loss='categorical_crossentropy',optimizer=Adam(0.001),metrics=['accuracy'])\n",
        " \n",
        "    self.model.summary()\n",
        "\n",
        "\n",
        "  def to_categorical(self, sequences, categories):\n",
        "    '''\n",
        "    Conversion into One Hot encoded vectors\n",
        "    '''\n",
        "    cat_sequences = []\n",
        "    for s in sequences:\n",
        "        cats = []\n",
        "        for item in s:\n",
        "            cats.append(np.zeros(categories))\n",
        "            cats[-1][item] = 1.0\n",
        "        cat_sequences.append(cats)\n",
        "    return np.array(cat_sequences)\n",
        "  \n",
        "\n",
        "  def calc_tag_metrics(self):\n",
        "        '''\n",
        "        Calculate the per-POS accuracy for all the tags in the tag-set\n",
        "        '''\n",
        "        counter_dict = defaultdict(lambda: defaultdict(lambda:0))\n",
        "        \n",
        "        for i in range(len(self.test_dataY)):\n",
        "            for j in range(len(self.test_dataY[i])):\n",
        "                if(self.test_dataY[i][j] == self.test_pred_dataY[i][j]):\n",
        "                    counter_dict[self.test_dataY[i][j]]['TP'] += 1\n",
        "                else:\n",
        "                    counter_dict[self.test_dataY[i][j]]['FN']    += 1\n",
        "                    counter_dict[self.test_pred_dataY[i][j]]['FP'] += 1\n",
        "        \n",
        "        for tag in counter_dict.keys():\n",
        "            counter_dict[tag]['TN'] = TOTAL_TAGGED_WORDS - counter_dict[tag]['TP']- counter_dict[tag]['FN'] - counter_dict[tag]['FP']\n",
        "        \n",
        "        for tag in counter_dict.keys():\n",
        "            self.tag_metrics[tag]['Precision'] = counter_dict[tag]['TP']/(counter_dict[tag]['TP']+counter_dict[tag]['FP'])\n",
        "            self.tag_metrics[tag]['Recall'] = counter_dict[tag]['TP']/(counter_dict[tag]['TP']+counter_dict[tag]['FN'])\n",
        "            self.tag_metrics[tag]['F1_score'] = 2*(self.tag_metrics[tag]['Precision']*self.tag_metrics[tag]['Recall'])/(self.tag_metrics[tag]['Precision']+self.tag_metrics[tag]['Recall'])\n",
        "            self.tag_metrics[tag]['Accuracy'] = (counter_dict[tag]['TP']+ counter_dict[tag]['TN']) / TOTAL_TAGGED_WORDS\n",
        "        \n",
        "        \n",
        "  def generate_confusion_matrix(self):\n",
        "      '''\n",
        "      Generate confusion matrix for the particular fold\n",
        "      '''\n",
        "      CM = ConfusionMatrix(list(chain.from_iterable(self.test_dataY)) ,list(chain.from_iterable(self.test_pred_dataY)))\n",
        "      print(CM)\n",
        "        \n",
        "  \n",
        "\n",
        "\n",
        "  def print_sample(self):\n",
        "      '''\n",
        "      Prints a sample of n = 5 actual and predicted tagged sentences for reference\n",
        "      '''\n",
        "      for i in range(5):\n",
        "        print(\"Actual :\",self.test_dataY[i])\n",
        "        print(\"Predicted :\",self.test_pred_dataY[i])\n",
        "        \n",
        "  def get_tag_metrics(self):\n",
        "        '''\n",
        "        Prints the per POS precision,recall and F1 score of predicted tags\n",
        "        '''\n",
        "        \n",
        "        print (\"{:<10} {:<10} {:<10} {:<10} {:<10}\".format('TAG', 'PRECISION', 'RECALL','F1_SCORE','ACCURACY'))\n",
        "        \n",
        "        for key in self.tag_metrics.keys():\n",
        "            precision = str(round(self.tag_metrics[key]['Precision'], 2))\n",
        "            recall    = str(round(self.tag_metrics[key]['Recall'], 2))\n",
        "            F1_score  = str(round(self.tag_metrics[key]['F1_score'], 2))\n",
        "            accuracy  = str(round(self.tag_metrics[key]['Accuracy'], 2))\n",
        "            print (\"{:<10} {:<10} {:<10} {:<10} {:<10}\".format(key, precision,recall,F1_score,accuracy)) \n",
        "\n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zlySRB10flqy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "Calculate average of scores obtained using 5-fold cross validation\n",
        "'''\n",
        "\n",
        "def avg(dict_list,folds):\n",
        "    avg_dict = defaultdict(lambda: defaultdict(lambda:0))\n",
        "    for d in dict_list:\n",
        "        for tag in d.keys():\n",
        "            avg_dict[tag]['Precision'] += d[tag]['Precision']/folds\n",
        "            avg_dict[tag]['Recall'] += d[tag]['Recall']/folds\n",
        "            avg_dict[tag]['F1_score'] += d[tag]['F1_score']/folds\n",
        "            avg_dict[tag]['Accuracy'] += d[tag]['Accuracy']/folds\n",
        "            \n",
        "    print (\"{:<10} {:<10} {:<10} {:<10} \".format('TAG', 'PRECISION', 'RECALL','F1_SCORE'))\n",
        "        \n",
        "    for key in avg_dict.keys():\n",
        "        precision = str(round(avg_dict[key]['Precision'], 2))\n",
        "        recall    = str(round(avg_dict[key]['Recall'], 2))\n",
        "        F1_score  = str(round(avg_dict[key]['F1_score'], 2))\n",
        "        accuracy  = str(round(avg_dict[key]['Accuracy'], 2))\n",
        "        print (\"{:<10} {:<10} {:<10} {:<10} \".format(key, precision,recall,F1_score))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AMJACvcPOLy9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e2ac80b8-ef15-482c-c101-fa24c3b39403"
      },
      "source": [
        "'''\n",
        "Finding accuracies via 5 fold cross validation\n",
        "'''\n",
        "\n",
        "sentences, sentence_tags = separate_tags(tagged_sentences)\n",
        "\n",
        "sent = np.array(sentences,dtype=object)\n",
        "sent_tags = np.array(sentence_tags,dtype=object)\n",
        "kfold = KFold(n_splits=5,shuffle=True)\n",
        "tag_metric_list = []\n",
        "predicted_tags_list = []\n",
        "actual_tags_list = []\n",
        "overall_accuracy = 0\n",
        "\n",
        "for train, test in kfold.split(sent):\n",
        "    train_sentences = list(sent[train])\n",
        "    test_sentences = list(sent[test])\n",
        "    train_tags = list(sent_tags[train])\n",
        "    test_tags = list(sent_tags[test])                 \n",
        "    lstm = BiLSTM_POS(train_sentences,test_sentences,train_tags, test_tags)\n",
        "    lstm.generate()\n",
        "    tag_metric_list.append(lstm.tag_metrics)\n",
        "    predicted_tags_list.extend(list(chain.from_iterable(lstm.test_pred_dataY)))\n",
        "    actual_tags_list.extend(list(chain.from_iterable(lstm.test_dataY)))\n",
        "    #overall_accuracy += lstm.accuracy()/5\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 180, 128)          5758976   \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, 180, 512)          788480    \n",
            "_________________________________________________________________\n",
            "time_distributed (TimeDistri (None, 180, 13)           6669      \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 180, 13)           0         \n",
            "=================================================================\n",
            "Total params: 6,554,125\n",
            "Trainable params: 6,554,125\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/4\n",
            "323/323 [==============================] - 925s 3s/step - loss: 0.2232 - accuracy: 0.9340 - val_loss: 0.0428 - val_accuracy: 0.9881\n",
            "Epoch 2/4\n",
            "323/323 [==============================] - 943s 3s/step - loss: 0.0215 - accuracy: 0.9943 - val_loss: 0.0127 - val_accuracy: 0.9962\n",
            "Epoch 3/4\n",
            "323/323 [==============================] - 945s 3s/step - loss: 0.0088 - accuracy: 0.9973 - val_loss: 0.0108 - val_accuracy: 0.9964\n",
            "Epoch 4/4\n",
            "323/323 [==============================] - 948s 3s/step - loss: 0.0065 - accuracy: 0.9980 - val_loss: 0.0105 - val_accuracy: 0.9967\n",
            "WARNING:tensorflow:From <ipython-input-5-b66b63a63f37>:102: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
            "Instructions for updating:\n",
            "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 180, 128)          5756800   \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, 180, 512)          788480    \n",
            "_________________________________________________________________\n",
            "time_distributed_1 (TimeDist (None, 180, 13)           6669      \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 180, 13)           0         \n",
            "=================================================================\n",
            "Total params: 6,551,949\n",
            "Trainable params: 6,551,949\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/4\n",
            "323/323 [==============================] - 969s 3s/step - loss: 0.2214 - accuracy: 0.9367 - val_loss: 0.0397 - val_accuracy: 0.9895\n",
            "Epoch 2/4\n",
            "323/323 [==============================] - 943s 3s/step - loss: 0.0201 - accuracy: 0.9946 - val_loss: 0.0126 - val_accuracy: 0.9962\n",
            "Epoch 3/4\n",
            "323/323 [==============================] - 936s 3s/step - loss: 0.0087 - accuracy: 0.9974 - val_loss: 0.0110 - val_accuracy: 0.9965\n",
            "Epoch 4/4\n",
            "323/323 [==============================] - 935s 3s/step - loss: 0.0063 - accuracy: 0.9980 - val_loss: 0.0103 - val_accuracy: 0.9967\n",
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, 180, 128)          5767424   \n",
            "_________________________________________________________________\n",
            "bidirectional_2 (Bidirection (None, 180, 512)          788480    \n",
            "_________________________________________________________________\n",
            "time_distributed_2 (TimeDist (None, 180, 13)           6669      \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 180, 13)           0         \n",
            "=================================================================\n",
            "Total params: 6,562,573\n",
            "Trainable params: 6,562,573\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/4\n",
            "323/323 [==============================] - 947s 3s/step - loss: 0.2006 - accuracy: 0.9425 - val_loss: 0.0279 - val_accuracy: 0.9929\n",
            "Epoch 2/4\n",
            "323/323 [==============================] - 945s 3s/step - loss: 0.0166 - accuracy: 0.9954 - val_loss: 0.0120 - val_accuracy: 0.9962\n",
            "Epoch 3/4\n",
            "323/323 [==============================] - 930s 3s/step - loss: 0.0083 - accuracy: 0.9975 - val_loss: 0.0106 - val_accuracy: 0.9966\n",
            "Epoch 4/4\n",
            "323/323 [==============================] - 932s 3s/step - loss: 0.0061 - accuracy: 0.9981 - val_loss: 0.0104 - val_accuracy: 0.9967\n",
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_3 (Embedding)      (None, 172, 128)          5773568   \n",
            "_________________________________________________________________\n",
            "bidirectional_3 (Bidirection (None, 172, 512)          788480    \n",
            "_________________________________________________________________\n",
            "time_distributed_3 (TimeDist (None, 172, 13)           6669      \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 172, 13)           0         \n",
            "=================================================================\n",
            "Total params: 6,568,717\n",
            "Trainable params: 6,568,717\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/4\n",
            "323/323 [==============================] - 888s 3s/step - loss: 0.2094 - accuracy: 0.9407 - val_loss: 0.0287 - val_accuracy: 0.9928\n",
            "Epoch 2/4\n",
            "323/323 [==============================] - 887s 3s/step - loss: 0.0174 - accuracy: 0.9951 - val_loss: 0.0125 - val_accuracy: 0.9960\n",
            "Epoch 3/4\n",
            "323/323 [==============================] - 886s 3s/step - loss: 0.0089 - accuracy: 0.9973 - val_loss: 0.0110 - val_accuracy: 0.9964\n",
            "Epoch 4/4\n",
            "323/323 [==============================] - 889s 3s/step - loss: 0.0065 - accuracy: 0.9979 - val_loss: 0.0106 - val_accuracy: 0.9965\n",
            "Model: \"sequential_9\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_4 (Embedding)      (None, 180, 128)          5781376   \n",
            "_________________________________________________________________\n",
            "bidirectional_4 (Bidirection (None, 180, 512)          788480    \n",
            "_________________________________________________________________\n",
            "time_distributed_4 (TimeDist (None, 180, 13)           6669      \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 180, 13)           0         \n",
            "=================================================================\n",
            "Total params: 6,576,525\n",
            "Trainable params: 6,576,525\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/4\n",
            "323/323 [==============================] - 935s 3s/step - loss: 0.2162 - accuracy: 0.9360 - val_loss: 0.0397 - val_accuracy: 0.9899\n",
            "Epoch 2/4\n",
            "323/323 [==============================] - 933s 3s/step - loss: 0.0211 - accuracy: 0.9943 - val_loss: 0.0130 - val_accuracy: 0.9960\n",
            "Epoch 3/4\n",
            "323/323 [==============================] - 932s 3s/step - loss: 0.0093 - accuracy: 0.9972 - val_loss: 0.0112 - val_accuracy: 0.9963\n",
            "Epoch 4/4\n",
            "323/323 [==============================] - 929s 3s/step - loss: 0.0069 - accuracy: 0.9978 - val_loss: 0.0107 - val_accuracy: 0.9966\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30Rng-Q3zJFP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "Calculate average of scores obtained using 5-fold cross validation\n",
        "'''\n",
        "\n",
        "def avg(tag_indexes,dict_list,folds):\n",
        "    avg_dict = defaultdict(lambda: defaultdict(lambda:0))\n",
        "    for d in dict_list:\n",
        "        for tag in d.keys():\n",
        "            avg_dict[tag]['Precision'] += d[tag]['Precision']/folds\n",
        "            avg_dict[tag]['Recall'] += d[tag]['Recall']/folds\n",
        "            avg_dict[tag]['F1_score'] += d[tag]['F1_score']/folds\n",
        "            avg_dict[tag]['Accuracy'] += d[tag]['Accuracy']/folds\n",
        "            \n",
        "    print (\"{:<10} {:<10} {:<10} {:<10} \".format('TAG', 'PRECISION', 'RECALL','F1_SCORE'))\n",
        "        \n",
        "    for key in avg_dict.keys():\n",
        "        precision = str(round(avg_dict[key]['Precision'], 2))\n",
        "        recall    = str(round(avg_dict[key]['Recall'], 2))\n",
        "        F1_score  = str(round(avg_dict[key]['F1_score'], 2))\n",
        "        accuracy  = str(round(avg_dict[key]['Accuracy'], 2))\n",
        "        print (\"{:<10} {:<10} {:<10} {:<10} \".format(tag_indexes[key], precision,recall,F1_score))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9VVSrqZxzcxp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def conversion_to_tag(tag_index, predicted_tags_list, actual_tags_list):\n",
        "    '''\n",
        "    Conversion of Tags from integer to words (0->PAD)\n",
        "    '''\n",
        "    tag_indexes = {}\n",
        "    for i in tag_index:\n",
        "      j = tag_index[i]\n",
        "      tag_indexes[j]=i\n",
        "    tag_indexes[0] = 'PAD'\n",
        "    predicted_tags =[]\n",
        "    actual_tags = []\n",
        "\n",
        "    for i in range(len(predicted_tags_list)):\n",
        "        predicted_tags.append(tag_indexes[predicted_tags_list[i]])\n",
        "\n",
        "    for i in range(len(actual_tags_list)):\n",
        "      actual_tags.append(tag_indexes[actual_tags_list[i]])\n",
        "\n",
        "    return tag_indexes, predicted_tags, actual_tags"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oyOvj9EykYA0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 638
        },
        "outputId": "588682b9-9bac-49ea-e9c8-51d6e036a26f"
      },
      "source": [
        "# Reporting precision,recall,F1 score \n",
        "unique_tags = set([]) \n",
        "for tags in sentence_tags:\n",
        "      for tag in tags:\n",
        "        unique_tags.add(tag)\n",
        "\n",
        "tag_index = {}\n",
        "\n",
        "for i, t in enumerate(list(unique_tags)):\n",
        "  tag_index[t]=i+1\n",
        "tag_index['PAD'] = 0\n",
        "\n",
        "tag_indexes, predicted, actual = conversion_to_tag(tag_index, predicted_tags_list, actual_tags_list)\n",
        "\n",
        "avg(tag_indexes, tag_metric_list,5)\n",
        "\n",
        "# Printing final confusion matrix\n",
        "'''\n",
        "NOTE : This is the confusion matrix plotted over total of all predictions obtained from the FOLDS=5 folds\n",
        "Divide by FOLDS in case average is required\n",
        "'''\n",
        "\n",
        "                \n",
        "\n",
        "print(ConfusionMatrix(predicted,actual))\n",
        "\n",
        "# Printing overall accuracy \n",
        "\n",
        "#print(\"OVERALL ACCURACY :\",round(overall_accuracy,4)*100,\"%\")\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TAG        PRECISION  RECALL     F1_SCORE   \n",
            "DET        0.99       0.99       0.99       \n",
            "NOUN       0.96       0.98       0.97       \n",
            "ADJ        0.94       0.91       0.92       \n",
            "VERB       0.97       0.97       0.97       \n",
            "ADP        0.97       0.98       0.98       \n",
            ".          1.0        1.0        1.0        \n",
            "PAD        1.0        1.0        1.0        \n",
            "PRT        0.94       0.91       0.93       \n",
            "NUM        0.97       0.92       0.95       \n",
            "ADV        0.94       0.92       0.93       \n",
            "PRON       0.99       0.98       0.99       \n",
            "CONJ       0.99       1.0        0.99       \n",
            "X          0.86       0.23       0.36       \n",
            "     |                                       C               N                       P               V         |\n",
            "     |               A       A       A       O       D       O       N       P       R       P       E         |\n",
            "     |               D       D       D       N       E       U       U       A       O       R       R         |\n",
            "     |       .       J       P       V       J       T       N       M       D       N       T       B       X |\n",
            "-----+---------------------------------------------------------------------------------------------------------+\n",
            "   . | <147562>      .      20       .       .       .       1       .       .       .       .       .      24 |\n",
            " ADJ |       .  <76237>     46    1671       .       1    2453     101       .       2     106     507      29 |\n",
            " ADP |       .      69 <141939>   1170       8     588      59       3       .     296    1834     127      27 |\n",
            " ADV |       .    1708     802  <51785>     64      88     185      10       .      11     269     139      44 |\n",
            "CONJ |       .       .     149     121  <38040>     58       2       .       .       .       .       1       5 |\n",
            " DET |       .       .     275     169      33 <135957>    108       1       .     509       2       .      19 |\n",
            "NOUN |       .    4889      48     670       2      12 <268700>    871      77      28     221    4850     756 |\n",
            " NUM |       .       7       1       3       1       5     349  <13742>      .       .       2       4      31 |\n",
            " PAD |       .       3       .       1       .       .      52       2<9068192>      .       .       8       4 |\n",
            "PRON |       .       .      42       2       1     300      22       1       .  <48484>      4       1       9 |\n",
            " PRT |       .      49    1350     361       1       1      37       4       .       1  <27298>      5       5 |\n",
            "VERB |       .     754      93     284       .       8    3544     138       3       3      89 <177105>    117 |\n",
            "   X |       .       5       .       2       .       1      45       1       .       .       3       2    <316>|\n",
            "-----+---------------------------------------------------------------------------------------------------------+\n",
            "(row = reference; col = test)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bcCjkAYT1JFt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "To calculate the overall accuracy for the data\n",
        "'''\n",
        "class overall_accuracy:\n",
        "    \n",
        "    def __init__(self, test_actual_tags, test_predicted_tags):\n",
        "        '''\n",
        "        The test_actual_tags contains actual tags for the above setences.\n",
        "        The test_predicted_tags contains predicted tags for the test data.\n",
        "        '''\n",
        "        self.counter_dict = defaultdict(lambda: defaultdict(lambda:0))\n",
        "        self.test_actual_tags = test_actual_tags\n",
        "        self.test_predicted_tags = test_predicted_tags\n",
        "        self.tag_metrics = defaultdict(lambda: defaultdict(lambda:0))\n",
        "            \n",
        "    def calc_tag_metrics(self):\n",
        "        '''\n",
        "        Calculate the per-POS accuracy for all the tags in the tag-set\n",
        "        '''\n",
        "        counter_dict = defaultdict(lambda: defaultdict(lambda:0))\n",
        "        \n",
        "        for i in range(len(self.test_actual_tags)):\n",
        "            \n",
        "            if(self.test_actual_tags[i] == self.test_predicted_tags[i]):\n",
        "                counter_dict[self.test_actual_tags[i]]['TP'] += 1\n",
        "            else:\n",
        "                counter_dict[self.test_actual_tags[i]]['FN']    += 1\n",
        "                counter_dict[self.test_predicted_tags[i]]['FP'] += 1\n",
        "        \n",
        "        for tag in counter_dict.keys():\n",
        "            counter_dict[tag]['TN'] = TOTAL_TAGGED_WORDS - counter_dict[tag]['TP']- counter_dict[tag]['FN'] - counter_dict[tag]['FP']\n",
        "        \n",
        "        for tag in counter_dict.keys():\n",
        "            try:\n",
        "                self.tag_metrics[tag]['Precision'] = counter_dict[tag]['TP']/(counter_dict[tag]['TP']+counter_dict[tag]['FP'])\n",
        "                self.tag_metrics[tag]['Recall'] = counter_dict[tag]['TP']/(counter_dict[tag]['TP']+counter_dict[tag]['FN'])\n",
        "                self.tag_metrics[tag]['F1_score'] = 2*(self.tag_metrics[tag]['Precision']*self.tag_metrics[tag]['Recall'])/(self.tag_metrics[tag]['Precision']+self.tag_metrics[tag]['Recall'])\n",
        "                self.tag_metrics[tag]['Accuracy'] = (counter_dict[tag]['TP']+ counter_dict[tag]['TN']) / TOTAL_TAGGED_WORDS\n",
        "            except ZeroDivisionError:\n",
        "                continue               \n",
        "        self.counter_dict =  counter_dict\n",
        "        \n",
        "        \n",
        "    def accuracy(self):\n",
        "        '''\n",
        "        Calculate average accuracy score\n",
        "        '''\n",
        "        self.calc_tag_metrics()\n",
        "        TP =0\n",
        "        FN =0\n",
        "        FP =0\n",
        "        for tag in self.counter_dict.keys():\n",
        "            if(tag != 'PAD'):\n",
        "              TP += self.counter_dict[tag]['TP']\n",
        "              FN += self.counter_dict[tag]['FN']\n",
        "              FP += self.counter_dict[tag]['FP']\n",
        "        \n",
        "        return TP/(FN+TP)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bNyyPXd92sla",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e4dd1e17-7634-4999-ec33-ea47b3ed1093"
      },
      "source": [
        "'''\n",
        "Printing the overall accuracy of the model\n",
        "'''\n",
        "LSTM_acc= overall_accuracy(actual, predicted)\n",
        "overall_acc = LSTM_acc.accuracy()\n",
        "print(\"OVERALL ACCURACY :\",round(overall_acc,4)*100,\"%\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "OVERALL ACCURACY : 97.07000000000001 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-y6c5Ht7Ma49",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}