{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /home/ankita/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     /home/ankita/nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n",
      "[nltk_data] Downloading package tagsets to /home/ankita/nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('brown')\n",
    "nltk.download('universal_tagset')\n",
    "nltk.download('tagsets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.utils import resample #downsample the dataset\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV \n",
    "from sklearn.preprocessing import scale #scale and center data\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import brown #Brown Corpus \n",
    "from sklearn import svm, datasets\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder,LabelBinarizer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from gensim.models import Word2Vec\n",
    "import gensim\n",
    "from nltk.tag import AffixTagger \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import preprocessing\n",
    "from nltk import ConfusionMatrix\n",
    "from itertools import chain \n",
    "from sklearn.ensemble import BaggingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVM_metrics():\n",
    "    \n",
    "    def __init__(self, test_actual_tags, test_predicted_tags):\n",
    "        '''\n",
    "        The test_actual_tags contains actual tags for the above setences.\n",
    "        The test_predicted_tags contains predicted tags for the test data.\n",
    "        '''\n",
    "        self.counter_dict = defaultdict(lambda: defaultdict(lambda:0))\n",
    "        self.test_actual_tags = test_actual_tags\n",
    "        self.test_predicted_tags = test_predicted_tags\n",
    "        self.tag_metrics = defaultdict(lambda: defaultdict(lambda:0))\n",
    "            \n",
    "    def calc_tag_metrics(self):\n",
    "        '''\n",
    "        Calculate the per-POS accuracy for all the tags in the tag-set\n",
    "        '''\n",
    "        counter_dict = defaultdict(lambda: defaultdict(lambda:0))\n",
    "        \n",
    "        for i in range(len(self.test_actual_tags)):\n",
    "            \n",
    "            if(self.test_actual_tags[i] == self.test_predicted_tags[i]):\n",
    "                counter_dict[self.test_actual_tags[i]]['TP'] += 1\n",
    "            else:\n",
    "                counter_dict[self.test_actual_tags[i]]['FN']    += 1\n",
    "                counter_dict[self.test_predicted_tags[i]]['FP'] += 1\n",
    "        \n",
    "        for tag in counter_dict.keys():\n",
    "            counter_dict[tag]['TN'] = TOTAL_TAGGED_WORDS - counter_dict[tag]['TP']- counter_dict[tag]['FN'] - counter_dict[tag]['FP']\n",
    "        \n",
    "        for tag in counter_dict.keys():\n",
    "            try:\n",
    "                self.tag_metrics[tag]['Precision'] = counter_dict[tag]['TP']/(counter_dict[tag]['TP']+counter_dict[tag]['FP'])\n",
    "                self.tag_metrics[tag]['Recall'] = counter_dict[tag]['TP']/(counter_dict[tag]['TP']+counter_dict[tag]['FN'])\n",
    "                self.tag_metrics[tag]['F1_score'] = 2*(self.tag_metrics[tag]['Precision']*self.tag_metrics[tag]['Recall'])/(self.tag_metrics[tag]['Precision']+self.tag_metrics[tag]['Recall'])\n",
    "                self.tag_metrics[tag]['Accuracy'] = (counter_dict[tag]['TP']+ counter_dict[tag]['TN']) / TOTAL_TAGGED_WORDS\n",
    "            except ZeroDivisionError:\n",
    "                continue               \n",
    "        self.counter_dict =  counter_dict\n",
    "        \n",
    "    def generate_confusion_matrix(self):\n",
    "        '''\n",
    "        Generate confusion matrix for the particular fold\n",
    "        '''\n",
    "        CM = ConfusionMatrix(self.test_actual_tags ,self.test_predicted_tags)\n",
    "        print(CM)\n",
    "        \n",
    "    def accuracy(self):\n",
    "        '''\n",
    "        Calculate average accuracy score\n",
    "        '''\n",
    "        TP =0\n",
    "        FP =0\n",
    "        for tag in self.counter_dict.keys():\n",
    "            TP += self.counter_dict[tag]['TP']\n",
    "            FP += self.counter_dict[tag]['FP']\n",
    "        \n",
    "        return TP/(TP+FP)\n",
    "        \n",
    "        \n",
    "    def print_sample(self):\n",
    "        '''\n",
    "        Prints a sample of n = 5 actual and predicted tagged sentences for reference\n",
    "        '''\n",
    "        for i in range(5):\n",
    "            print(\"Actual :\",self.test_actual_tags[i])\n",
    "            print(\"Predicted :\",self.test_predicted_tags[i])\n",
    "        \n",
    "    def get_tag_metrics(self):\n",
    "        '''\n",
    "        Prints the per POS precision,recall and F1 score of predicted tags\n",
    "        '''\n",
    "        \n",
    "        print (\"{:<10} {:<10} {:<10} {:<10}\".format('TAG', 'PRECISION', 'RECALL','F1_SCORE'))\n",
    "        \n",
    "        for key in self.tag_metrics.keys():\n",
    "            precision = str(round(self.tag_metrics[key]['Precision'], 2))\n",
    "            recall    = str(round(self.tag_metrics[key]['Recall'], 2))\n",
    "            F1_score  = str(round(self.tag_metrics[key]['F1_score'], 2))\n",
    "            accuracy  = str(round(self.tag_metrics[key]['Accuracy'], 2))\n",
    "            print (\"{:<10} {:<10} {:<10} {:<10} \".format(key, precision,recall,F1_score)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Not using prefixes '"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Constant declarations'''\n",
    "\n",
    "UNIVERSAL_TAGSET =['NOUN', 'DET', 'ADJ', 'ADP', '.', 'VERB', 'CONJ', 'NUM', 'ADV', 'PRT', 'PRON', 'X']\n",
    "TOTAL_TAGGED_WORDS = len(brown.words())\n",
    "FOLDS = 5\n",
    "\n",
    "# Suffixes\n",
    "\n",
    "NOUN_SUFFIX =['eer','er','ion','ity','ment','ness','or','tion','ship','th']\n",
    "ADJ_SUFFIX = ['able','ible,','al','ant','ary','ful','ic','ious','ous','ive','less','y']\n",
    "VERB_SUFFIX = ['ed','en','er','ing','ise','ize']\n",
    "ADVERB_SUFFIX = ['ly','ward','wise']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Training Word2Vec model for word vectorisations using Brown corpus\n",
    "'''\n",
    "\n",
    "sentences = brown.sents()\n",
    "model = Word2Vec(sentences, min_count=1,size=50)\n",
    "wordvectors = model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_suffix(word,suffix):\n",
    "    '''\n",
    "    Utility function to check if a string in the array passed is a suffix or not\n",
    "    '''\n",
    "    val = False\n",
    "    for s in suffix:\n",
    "        val = val or word.endswith(s)\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_prefix(word,prefix):\n",
    "    '''\n",
    "    Utility function to check if a string in the array passed is a prefix or not\n",
    "    '''\n",
    "    val = False\n",
    "    for s in prefix:\n",
    "        val = val or word.startswith(s)\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_pos_suffix(word):\n",
    "    '''\n",
    "    Output predicted part of speech based on suffixes\n",
    "    '''\n",
    "    if check_suffix(word,NOUN_SUFFIX):\n",
    "        return \"Noun\"\n",
    "    elif check_suffix(word,ADJ_SUFFIX):\n",
    "        return \"Adj\"\n",
    "    elif check_suffix(word,VERB_SUFFIX):\n",
    "        return \"Verb\"\n",
    "    elif check_suffix(word,ADVERB_SUFFIX):\n",
    "        return \"Adverb\"\n",
    "    else: \n",
    "        return \"None\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def morphological_features(sentence_words, index):\n",
    "    \"\"\" \n",
    "    Extract morphological features of the word occuring in the sentences\n",
    "    \"\"\"\n",
    "    word = sentence_words[index]\n",
    "    return {\n",
    "        'total_words': len(sentence_words),\n",
    "        'word': word,\n",
    "        'is_first': index == 0,\n",
    "        'is_last': index == len(sentence_words) - 1,\n",
    "        'len_word': len(word),\n",
    "        'is_capitalized': word[0].upper() == word[0],\n",
    "        'all_capital': word.upper() == word,\n",
    "        'all_lower': word.lower() == word,\n",
    "        'suffix': output_pos_suffix(word.lower()),\n",
    "        'next_word_suffix': \"None\" if index == len(sentence_words) - 1 else output_pos_suffix(sentence_words[index + 1].lower()),\n",
    "        'prev_word_suffix': \"None\" if index == 0 else output_pos_suffix(sentence_words[index - 1].lower()),\n",
    "        'prev_word_len' : 0 if index == 0 else len(sentence_words[index - 1]),\n",
    "        'next_word_len' :0  if index == len(sentence_words) - 1 else len(sentence_words[index + 1]),\n",
    "        'prev_word': '' if index == 0 else sentence_words[index - 1],\n",
    "        'next_word': '' if index == len(sentence_words) - 1 else sentence_words[index + 1],\n",
    "        'similarity_prev': 0 if index == 0 else model.similarity(word,sentence_words[index - 1]),\n",
    "        'similarity_next': 0 if index == len(sentence_words) - 1 else model.similarity(word,sentence_words[index + 1]),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Getting brown corpus tagged sentences \n",
    "'''\n",
    "sent_tag = brown.tagged_sents(tagset='universal')\n",
    "mod_sent_tag=[]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Dividing the test data into test words and test tags\n",
    "'''\n",
    "words=[]\n",
    "tags=[]\n",
    "for s in sent_tag:\n",
    "  temp_word=[]\n",
    "  temp_tag=[]\n",
    "  for (w,t) in s:\n",
    "    #temp_word.append(w.lower())\n",
    "    temp_word.append(w)\n",
    "    temp_tag.append(t)\n",
    "  words.append(temp_word)\n",
    "  tags.append(temp_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ankita/.local/lib/python3.6/site-packages/ipykernel_launcher.py:23: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "/home/ankita/.local/lib/python3.6/site-packages/ipykernel_launcher.py:22: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Getting the morphological features of the word and appending to one list \n",
    "'''\n",
    "full_data_list =[]\n",
    "for i in range(len(words)):\n",
    "    for j in range(len(words[i])):\n",
    "        dd = morphological_features(words[i],j)\n",
    "        l =list( dd.values())\n",
    "        l.extend([tags[i][j]])\n",
    "        full_data_list.append(l)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Converting the list to dataframe for application to SVM\n",
    "'''\n",
    "df = pd.DataFrame(full_data_list, columns =['total_words', 'word', 'is_first', 'is_last','len_word', 'is_capitalized', 'is_all_caps', 'is_all_lower', 'suffix','next_word_suffix','prev_word_suffix','prev_word_len','next_word_len','prev_word', 'next_word','sim_prev','sim_next','pred'])\n",
    "#df = df.head(200000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Converting True/False values to binary format\n",
    "'''\n",
    "\n",
    "df['is_first']=  df[\"is_first\"].astype(int)\n",
    "df['is_last']=  df[\"is_last\"].astype(int)\n",
    "df['is_capitalized']=  df[\"is_capitalized\"].astype(int)\n",
    "df['is_all_caps']=  df[\"is_all_caps\"].astype(int)\n",
    "df['is_all_lower']=  df[\"is_all_lower\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_words</th>\n",
       "      <th>word</th>\n",
       "      <th>is_first</th>\n",
       "      <th>is_last</th>\n",
       "      <th>len_word</th>\n",
       "      <th>is_capitalized</th>\n",
       "      <th>is_all_caps</th>\n",
       "      <th>is_all_lower</th>\n",
       "      <th>suffix</th>\n",
       "      <th>next_word_suffix</th>\n",
       "      <th>prev_word_suffix</th>\n",
       "      <th>prev_word_len</th>\n",
       "      <th>next_word_len</th>\n",
       "      <th>prev_word</th>\n",
       "      <th>next_word</th>\n",
       "      <th>sim_prev</th>\n",
       "      <th>sim_next</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25</td>\n",
       "      <td>The</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td></td>\n",
       "      <td>Fulton</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.347621</td>\n",
       "      <td>DET</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25</td>\n",
       "      <td>Fulton</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>Adj</td>\n",
       "      <td>None</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>The</td>\n",
       "      <td>County</td>\n",
       "      <td>0.347621</td>\n",
       "      <td>0.821048</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25</td>\n",
       "      <td>County</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Adj</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>Fulton</td>\n",
       "      <td>Grand</td>\n",
       "      <td>0.821048</td>\n",
       "      <td>0.950832</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25</td>\n",
       "      <td>Grand</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>Adj</td>\n",
       "      <td>Adj</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>County</td>\n",
       "      <td>Jury</td>\n",
       "      <td>0.950832</td>\n",
       "      <td>0.934176</td>\n",
       "      <td>ADJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25</td>\n",
       "      <td>Jury</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Adj</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>Grand</td>\n",
       "      <td>said</td>\n",
       "      <td>0.934176</td>\n",
       "      <td>0.408430</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   total_words    word  is_first  is_last  len_word  is_capitalized  \\\n",
       "0           25     The         1        0         3               1   \n",
       "1           25  Fulton         0        0         6               1   \n",
       "2           25  County         0        0         6               1   \n",
       "3           25   Grand         0        0         5               1   \n",
       "4           25    Jury         0        0         4               1   \n",
       "\n",
       "   is_all_caps  is_all_lower suffix next_word_suffix prev_word_suffix  \\\n",
       "0            0             0   None             None             None   \n",
       "1            0             0   None              Adj             None   \n",
       "2            0             0    Adj             None             None   \n",
       "3            0             0   None              Adj              Adj   \n",
       "4            0             0    Adj             None             None   \n",
       "\n",
       "   prev_word_len  next_word_len prev_word next_word  sim_prev  sim_next  pred  \n",
       "0              0              6              Fulton  0.000000  0.347621   DET  \n",
       "1              3              6       The    County  0.347621  0.821048  NOUN  \n",
       "2              6              5    Fulton     Grand  0.821048  0.950832  NOUN  \n",
       "3              6              4    County      Jury  0.950832  0.934176   ADJ  \n",
       "4              5              4     Grand      said  0.934176  0.408430  NOUN  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Seggregating the dataset to features and output (X and Y)\n",
    "'''\n",
    "\n",
    "X = df.drop('pred',axis =1).copy()\n",
    "Y = df['pred'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Applying word2vec embeddings to all the words that occur in the frame\n",
    "'''\n",
    "\n",
    "X['word'] = X['word'].apply(lambda x : wordvectors.word_vec(x))\n",
    "X['prev_word'] = X['prev_word'].apply(lambda x : wordvectors.word_vec(x) if (x != '') else [''])\n",
    "X['next_word']= X['next_word'].apply(lambda x : wordvectors.word_vec(x) if (x != '') else [''])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Breaking down the word vector list values to individual columns\n",
    "'''\n",
    "\n",
    "T = pd.DataFrame(X.word.values.tolist()).add_prefix('word_') # Current Word vector\n",
    "P = pd.DataFrame(X.prev_word.values.tolist()).add_prefix('prev_') # previous word vector\n",
    "N = pd.DataFrame(X.next_word.values.tolist()).add_prefix('next_') # next word vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Concatenate word vectorisations \n",
    "Also drop string columns which are now vectorised\n",
    "'''\n",
    "\n",
    "X = pd.concat([X, T], axis=1) \n",
    "X = pd.concat([X, P], axis=1)\n",
    "X = pd.concat([X, N], axis=1)\n",
    "X = X.drop(['word', 'prev_word','next_word'], axis=1)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Do one hot encoding of required columns\n",
    "'''\n",
    "X_encoded = pd.get_dummies(X, columns = ['suffix','next_word_suffix','prev_word_suffix'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_words</th>\n",
       "      <th>is_first</th>\n",
       "      <th>is_last</th>\n",
       "      <th>len_word</th>\n",
       "      <th>is_capitalized</th>\n",
       "      <th>is_all_caps</th>\n",
       "      <th>is_all_lower</th>\n",
       "      <th>prev_word_len</th>\n",
       "      <th>next_word_len</th>\n",
       "      <th>sim_prev</th>\n",
       "      <th>...</th>\n",
       "      <th>next_word_suffix_Adj</th>\n",
       "      <th>next_word_suffix_Adverb</th>\n",
       "      <th>next_word_suffix_None</th>\n",
       "      <th>next_word_suffix_Noun</th>\n",
       "      <th>next_word_suffix_Verb</th>\n",
       "      <th>prev_word_suffix_Adj</th>\n",
       "      <th>prev_word_suffix_Adverb</th>\n",
       "      <th>prev_word_suffix_None</th>\n",
       "      <th>prev_word_suffix_Noun</th>\n",
       "      <th>prev_word_suffix_Verb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>0.347621</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>0.821048</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>0.950832</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0.934176</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 176 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   total_words  is_first  is_last  len_word  is_capitalized  is_all_caps  \\\n",
       "0           25         1        0         3               1            0   \n",
       "1           25         0        0         6               1            0   \n",
       "2           25         0        0         6               1            0   \n",
       "3           25         0        0         5               1            0   \n",
       "4           25         0        0         4               1            0   \n",
       "\n",
       "   is_all_lower  prev_word_len  next_word_len  sim_prev  ...  \\\n",
       "0             0              0              6  0.000000  ...   \n",
       "1             0              3              6  0.347621  ...   \n",
       "2             0              6              5  0.821048  ...   \n",
       "3             0              6              4  0.950832  ...   \n",
       "4             0              5              4  0.934176  ...   \n",
       "\n",
       "   next_word_suffix_Adj  next_word_suffix_Adverb  next_word_suffix_None  \\\n",
       "0                     0                        0                      1   \n",
       "1                     1                        0                      0   \n",
       "2                     0                        0                      1   \n",
       "3                     1                        0                      0   \n",
       "4                     0                        0                      1   \n",
       "\n",
       "   next_word_suffix_Noun  next_word_suffix_Verb  prev_word_suffix_Adj  \\\n",
       "0                      0                      0                     0   \n",
       "1                      0                      0                     0   \n",
       "2                      0                      0                     0   \n",
       "3                      0                      0                     1   \n",
       "4                      0                      0                     0   \n",
       "\n",
       "   prev_word_suffix_Adverb  prev_word_suffix_None  prev_word_suffix_Noun  \\\n",
       "0                        0                      1                      0   \n",
       "1                        0                      1                      0   \n",
       "2                        0                      1                      0   \n",
       "3                        0                      0                      0   \n",
       "4                        0                      1                      0   \n",
       "\n",
       "   prev_word_suffix_Verb  \n",
       "0                      0  \n",
       "1                      0  \n",
       "2                      0  \n",
       "3                      0  \n",
       "4                      0  \n",
       "\n",
       "[5 rows x 176 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_encoded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Encode POS tags to be predicted\n",
    "'''\n",
    "\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "encoder.fit(Y)\n",
    "Y_encoded = encoder.transform(Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0\n",
       "0  5\n",
       "1  6\n",
       "2  6\n",
       "3  1\n",
       "4  6"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_encoded = pd.DataFrame(Y_encoded)\n",
    "Y_encoded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Fill NaN with 0 \n",
    "'''\n",
    "X_encoded = X_encoded.apply(pd.to_numeric)\n",
    "X_encoded = X_encoded.fillna(-2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Dumping data to pickle file in case the kernel restarts\n",
    "'''\n",
    "X_encoded.to_pickle(\"x.pkl\")\n",
    "Y_encoded.to_pickle(\"y.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Reading back the encoded data.\n",
    "Start from here and load packages if kernel restarts\n",
    "'''\n",
    "X_encoded = pd.read_pickle(\"x.pkl\")\n",
    "Y_encoded = pd.read_pickle(\"y.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Seggregation for test sample so that we do not consider rows already used for train\n",
    "'''\n",
    "# X_encoded =X_encoded.tail(900000)\n",
    "# Y_encoded= Y_encoded.tail(900000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Scaling X_encoded values\n",
    "'''\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Seggregating into train and test sets (Apply 5 fold later)\n",
    "'''\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train_scaled, Y_encoded, train_size=0.8, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Getting 1000 rows for each tag and storing them in a dataframe\n",
    "Commented because we have not used this for final results\n",
    "'''\n",
    "'''\n",
    "df_train = pd.concat([pd.DataFrame(X_train), pd.DataFrame(y_train)],axis=1)\n",
    "df_equal_tag = pd.DataFrame() \n",
    "df =  df.sample(frac = 1)  #Shuffle dataframe\n",
    "df_tag_divided = [pd.DataFrame(y).head(10000) for x, y in df_train.groupby(y_train[0], as_index=False)]\n",
    "for i in range(len(df_tag_divided)):\n",
    "    df_equal_tag = df_equal_tag.append(df_tag_divided[i])\n",
    "X_train = X_train.head(20000)\n",
    "y_train = y_train.head(20000)\n",
    "X_test = X_test.head(1000)\n",
    "y_test = y_test.head(1000)\n",
    "df_equal_tag=df_equal_tag.dropna()\n",
    "X_train = df_equal_tag.iloc[:, :-1]\n",
    "y_train = df_equal_tag.iloc[:,-1]\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "y_train = pd.DataFrame(y_train)\n",
    "X_train\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=SVC(),\n",
       "             param_grid=[{'C': [1000], 'gamma': [0.001], 'kernel': ['rbf']}])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Training the SVM classifier \n",
    "'''\n",
    "\n",
    "# params_grid = [{'kernel': ['rbf'], 'gamma': [1e-3, 1e-4],\n",
    "#                      'C': [1, 10, 100, 1000]},\n",
    "#                     {'kernel': ['linear'], 'C': [1, 10, 100, 1000]}]\n",
    "            \n",
    "params_grid = [{'kernel': ['rbf'], 'gamma': [1e-3], 'C': [1000]}]\n",
    "\n",
    "svm_model = GridSearchCV(SVC(), params_grid, cv=5)\n",
    "# from sklearn.linear_model import SGDClassifier\n",
    "# model = SGDClassifier(loss=\"hinge\", penalty=\"l2\",max_iter=1000)\n",
    "# svm_model = BaggingClassifier(SVC(C=1000,\n",
    "#         cache_size=200,\n",
    "#         class_weight=None,\n",
    "#         coef0=0.0,\n",
    "#         degree=3,\n",
    "#         gamma=1e-3,\n",
    "#         kernel='rbf',\n",
    "#         max_iter=-1,\n",
    "#         probability=False,\n",
    "#         random_state=None,\n",
    "#         shrinking=True,\n",
    "#         tol=0.001,\n",
    "#         verbose=False,\n",
    "#         ))\n",
    "\n",
    "svm_model.fit(X_train,y_train)\n",
    "#model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score for training data: 0.9422437499999999 \n",
      "\n",
      "Best C: 1000 \n",
      "\n",
      "Best Kernel: rbf \n",
      "\n",
      "Best Gamma: 0.001 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "View the accuracy score\n",
    "''' \n",
    "print('Best score for training data:', svm_model.best_score_,\"\\n\") \n",
    "\n",
    "'''\n",
    "View the best parameters for the model found using grid search\n",
    "'''\n",
    "\n",
    "print('Best C:',svm_model.best_estimator_.C,\"\\n\") \n",
    "print('Best Kernel:',svm_model.best_estimator_.kernel,\"\\n\")\n",
    "print('Best Gamma:',svm_model.best_estimator_.gamma,\"\\n\")\n",
    "\n",
    "#final_model = svm_model.best_estimator_.fit(X_train,y_train)\n",
    "\n",
    "#Y_pred = final_model.predict(X_test)\n",
    "\n",
    "# print(\"MODEL SCORE: \",model.score(X_test,y_test))\n",
    "#Y_pred = svm_model.predict(X_test)\n",
    "Y_pred = svm_model.predict(X_train_scaled)\n",
    "# '''\n",
    "# Decoding the predicted labels\n",
    "# '''\n",
    "\n",
    "Y_pred_label = list(encoder.inverse_transform(Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Recalculating actual labels via decoding\n",
    "'''\n",
    "\n",
    "#Y_actual_label = list(encoder.inverse_transform(y_test))\n",
    "Y_actual_label = list(encoder.inverse_transform(Y_encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TAG        PRECISION  RECALL     F1_SCORE  \n",
      "NOUN       0.9        0.94       0.92       \n",
      ".          1.0        1.0        1.0        \n",
      "CONJ       0.99       0.99       0.99       \n",
      "PRON       0.99       0.96       0.97       \n",
      "VERB       0.96       0.94       0.95       \n",
      "DET        0.99       0.99       0.99       \n",
      "ADV        0.88       0.8        0.84       \n",
      "NUM        0.92       0.91       0.91       \n",
      "ADP        0.97       0.97       0.97       \n",
      "ADJ        0.76       0.77       0.77       \n",
      "PRT        0.92       0.87       0.89       \n",
      "X          0.43       0.09       0.14       \n",
      "     |                                  C             N             P             V        |\n",
      "     |             A      A      A      O      D      O      N      R      P      E        |\n",
      "     |             D      D      D      N      E      U      U      O      R      R        |\n",
      "     |      .      J      P      V      J      T      N      M      N      T      B      X |\n",
      "-----+-------------------------------------------------------------------------------------+\n",
      "   . |<116249>     .      .      .      .      .      3      4      .      .      .      . |\n",
      " ADJ |      . <49302>    78   1745      2      5  10831     54      .     24   1951     14 |\n",
      " ADP |     14    166<108783>   921     71    251    290     99    172   1335    332      1 |\n",
      " ADV |      .   4680   1075 <35727>    63    211   2325     47     35    204    445      2 |\n",
      "CONJ |      .     50     83    174 <29720>    43     44      1      .      3     10      . |\n",
      " DET |      .    110    314    154      9<105173>   129      4    252      .      9      4 |\n",
      "NOUN |      .   7353    120    924      5     94<193845>   553     23     46   2926     85 |\n",
      " NUM |      .    211      4     35      4      5    674  <9670>     .      .     80      1 |\n",
      "PRON |      .     73    229     69      2    624    625      1 <39148>     6     73      3 |\n",
      " PRT |      .    206   1333    679      .     28    663      2     22 <20656>   207      2 |\n",
      "VERB |      .   2280    275    321     11     31   5709     16     18    120<135005>    11 |\n",
      "   X |     23     32     27     10      7     17    824     13      6      3     41    <94>|\n",
      "-----+-------------------------------------------------------------------------------------+\n",
      "(row = reference; col = test)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "93.708"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Generating confusion matrix and printing accuracies\n",
    "'''\n",
    "metrics = SVM_metrics(Y_actual_label,Y_pred_label)\n",
    "metrics.calc_tag_metrics()\n",
    "metrics.get_tag_metrics()\n",
    "metrics.generate_confusion_matrix()\n",
    "metrics.accuracy()*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
