{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /home/ankita/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     /home/ankita/nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n",
      "[nltk_data] Downloading package tagsets to /home/ankita/nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('brown')\n",
    "nltk.download('universal_tagset')\n",
    "nltk.download('tagsets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import brown #Brown Corpus \n",
    "from collections import defaultdict \n",
    "from nltk.metrics import ConfusionMatrix\n",
    "from sklearn.model_selection import KFold\n",
    "from itertools import chain "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Constant declarations\n",
    "'''\n",
    "\n",
    "SMOOTHENING_FACTOR = 0.00001\n",
    "UNIVERSAL_TAGSET =['NOUN', 'DET', 'ADJ', 'ADP', '.', 'VERB', 'CONJ', 'NUM', 'ADV', 'PRT', 'PRON', 'X']\n",
    "TOTAL_TAGGED_WORDS = len(brown.words())\n",
    "FOLDS = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Calculate count of each tag for informational purposes\n",
    "'''\n",
    "br = brown.tagged_words(tagset='universal')\n",
    "count_tag = defaultdict(int)\n",
    "for i in range(len(br)):\n",
    "    count_tag[br[i][1]] +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HMM_POS():\n",
    "    \n",
    "    def __init__(self, train_data, test_data, token_tags_map):\n",
    "        '''\n",
    "        Initialising train and test data obtained after 5 fold cross validation\n",
    "        The train/test data contains tagged sentences picked up from brown corpus.\n",
    "        The token_tags_map contains words and their corresponding tag mappings which are possible\n",
    "        The test_sent contains only the sentences of the test data.\n",
    "        The test_actual_tags contains actual tags for the above setences.\n",
    "        The test_predicted_tags contains predicted tags for the test data.\n",
    "        The tag_metrics contains accuracy scores for all the tag values\n",
    "        The counter_dict contains values for TN,TP,FN,FP tag wise\n",
    "        '''\n",
    "        self.train = train_data\n",
    "        self.test  = test_data\n",
    "        self.emission_matrix = {}\n",
    "        self.transition_matrix = {}\n",
    "        self.token_tags_map = token_tags_map\n",
    "        self.test_sent = []\n",
    "        self.test_actual_tags = []\n",
    "        self.test_predicted_tags = []\n",
    "        self.tag_metrics = defaultdict(lambda: defaultdict(lambda:0))\n",
    "        self.counter_dict = defaultdict(lambda: defaultdict(lambda:0))\n",
    "        \n",
    "    def calc_emmision_matrix(self):\n",
    "        '''\n",
    "        Creating emmision matrix from train data\n",
    "        '''\n",
    "        counter_dict =defaultdict(lambda: defaultdict(lambda:0)) # Storing total count of words used with the tag for calculating probabitities\n",
    "        for data in self.train:\n",
    "            for word,tag in data:\n",
    "                counter_dict[word][tag] +=1\n",
    "        \n",
    "        for key in counter_dict.keys():\n",
    "            self.emission_matrix[key] ={}\n",
    "            for k in counter_dict[key].keys():\n",
    "                self.emission_matrix[key][k] = counter_dict[key][k]/sum(counter_dict[key].values())\n",
    "                \n",
    "    def calc_transition_matrix(self):\n",
    "        '''\n",
    "        Creating transition matrix from train data using bigram probabilities\n",
    "        '''\n",
    "        counter_dict =defaultdict(lambda: defaultdict(lambda:0))\n",
    "        for data in self.train:\n",
    "            bigrams=list(nltk.bigrams(data)) # Generating bigrams from data (sentences)\n",
    "            for a,b in bigrams:\n",
    "                counter_dict[(a[1])][(b[1])] +=1\n",
    "        \n",
    "        for key in counter_dict.keys():\n",
    "            self.transition_matrix[key] ={}\n",
    "            for k in counter_dict[key].keys():\n",
    "                self.transition_matrix[key][k] = counter_dict[key][k]/sum(counter_dict[key].values())\n",
    "                \n",
    "                    \n",
    "    def viterbi_one_pass(self,sentence_split):\n",
    "        '''\n",
    "        Viterbi Algorithm applied to one sentence and corresponding states\n",
    "        '''\n",
    "        prob_state ={}\n",
    "        initial_prob_added =  False\n",
    "        # Storing initial probabilities for start of sentences first\n",
    "        \n",
    "        possible_tags = self.token_tags_map[sentence_split[1]] \n",
    "        prob_state[1] = {}\n",
    "        for tag in possible_tags:\n",
    "            #exists = tag in transition_matrix['^'] # Checking if tag exists in matrix\n",
    "            try:\n",
    "                prob_state[1][tag] = ['^',self.transition_matrix['^'][tag]*self.emission_matrix[tag][sentence_split[1]]]\n",
    "            except KeyError: # If tag does not exist \n",
    "                #print(tag)\n",
    "                prob_state[1][tag] = ['^',SMOOTHENING_FACTOR] # Assign default value\n",
    "        \n",
    "        # Iterating through the rest of the sentence\n",
    "        \n",
    "        for k in range(2,len(sentence_split)):\n",
    "            prob_state[k] = {}\n",
    "            prev_state  = list(prob_state[k-1].keys()) \n",
    "            curr_state  = self.token_tags_map[sentence_split[k]]     \n",
    "            for tag in curr_state:                             \n",
    "                all_states = []\n",
    "                for pt in prev_state: \n",
    "                    try:\n",
    "                        all_states.append(prob_state[k-1][pt][1]*self.transition_matrix[pt][tag]*self.emission_matrix[tag][sentence_split[k]])\n",
    "                    except KeyError:\n",
    "                        #print(tag,sentence_split[k])\n",
    "                        all_states.append(prob_state[k-1][pt][1]*SMOOTHENING_FACTOR)\n",
    "                max_in_current_level = all_states.index(max(all_states)) \n",
    "                prob_state[k][tag]=[prev_state[max_in_current_level],max(all_states)] #Stores the index as well as the tag seq with max probability\n",
    "        \n",
    "        # Back tracing to get the predicted tag sequence\n",
    "        \n",
    "        tags = ['<EOS>']\n",
    "        for itr in reversed(range(len(sentence_split))):\n",
    "            if(itr>0):\n",
    "                tags.append(prob_state[itr][tags[len(tags)-1]][0])\n",
    "        \n",
    "        return list(reversed(tags))\n",
    "                \n",
    "    def viterbi(self):\n",
    "        '''\n",
    "        Viterbi Algorithm applied on test data\n",
    "        '''\n",
    "        tag_seq = []\n",
    "        for data in self.test_sent:\n",
    "            tag_seq = self.viterbi_one_pass(data)\n",
    "            self.test_predicted_tags.append(tag_seq)\n",
    "        \n",
    "    def split_test(self):\n",
    "        '''\n",
    "        Split test data into sentences and corresponding tag lists.\n",
    "        This tag list is the actual_tags list that we need to compare with predicted tags\n",
    "        '''\n",
    "        sents = []\n",
    "        tags  = []\n",
    "        for data in self.test :\n",
    "            for word,tag in data:\n",
    "                sents.append(word)\n",
    "                tags.append(tag)\n",
    "            self.test_sent.append(sents)\n",
    "            self.test_actual_tags.append(tags)\n",
    "            sents = []\n",
    "            tags  = []\n",
    "    \n",
    "    def calculate(self):\n",
    "        '''\n",
    "        Perform all the computations required to get predictions\n",
    "        '''\n",
    "        self.calc_emmision_matrix()\n",
    "        self.calc_transition_matrix()\n",
    "        self.split_test()\n",
    "        self.viterbi()\n",
    "        self.calc_tag_metrics()\n",
    "    \n",
    "    def calc_tag_metrics(self):\n",
    "        '''\n",
    "        Calculate the per-POS accuracy for all the tags in the tag-set\n",
    "        '''\n",
    "        counter_dict = defaultdict(lambda: defaultdict(lambda:0))\n",
    "        \n",
    "        for i in range(len(self.test_actual_tags)):\n",
    "            for j in range(len(self.test_actual_tags[i])):\n",
    "                if(self.test_actual_tags[i][j] == self.test_predicted_tags[i][j]):\n",
    "                    counter_dict[self.test_actual_tags[i][j]]['TP'] += 1\n",
    "                else:\n",
    "                    counter_dict[self.test_actual_tags[i][j]]['FN']    += 1\n",
    "                    counter_dict[self.test_predicted_tags[i][j]]['FP'] += 1\n",
    "        \n",
    "        for tag in counter_dict.keys():\n",
    "            counter_dict[tag]['TN'] = TOTAL_TAGGED_WORDS - counter_dict[tag]['TP']- counter_dict[tag]['FN'] - counter_dict[tag]['FP']\n",
    "        \n",
    "        for tag in counter_dict.keys():\n",
    "            try:                \n",
    "                self.tag_metrics[tag]['Precision'] = counter_dict[tag]['TP']/(counter_dict[tag]['TP']+counter_dict[tag]['FP'])\n",
    "                self.tag_metrics[tag]['Recall'] = counter_dict[tag]['TP']/(counter_dict[tag]['TP']+counter_dict[tag]['FN'])\n",
    "                self.tag_metrics[tag]['F1_score'] = 2*(self.tag_metrics[tag]['Precision']*self.tag_metrics[tag]['Recall'])/(self.tag_metrics[tag]['Precision']+self.tag_metrics[tag]['Recall'])\n",
    "                self.tag_metrics[tag]['Accuracy'] = (counter_dict[tag]['TP']+ counter_dict[tag]['TN']) / TOTAL_TAGGED_WORDS\n",
    "            \n",
    "            except ZeroDivisionError:\n",
    "                continue\n",
    "                \n",
    "        self.counter_dict =  counter_dict\n",
    "        \n",
    "    def generate_confusion_matrix(self):\n",
    "        '''\n",
    "        Generate confusion matrix for the particular fold\n",
    "        '''\n",
    "        CM = ConfusionMatrix(list(chain.from_iterable(self.test_actual_tags)) ,list(chain.from_iterable(self.test_predicted_tags)))\n",
    "        print(CM)\n",
    "        \n",
    "    def accuracy(self):\n",
    "        '''\n",
    "        Calculate average accuracy score\n",
    "        '''\n",
    "        TP =0\n",
    "        FP =0\n",
    "        for tag in self.counter_dict.keys():\n",
    "            TP += self.counter_dict[tag]['TP']\n",
    "            FP += self.counter_dict[tag]['FP']\n",
    "        \n",
    "        return TP/(TP+FP)\n",
    "        \n",
    "        \n",
    "    def print_sample(self):\n",
    "        '''\n",
    "        Prints a sample of n = 5 actual and predicted tagged sentences for reference\n",
    "        '''\n",
    "        for i in range(5):\n",
    "            print(\"Actual :\",self.test_actual_tags[i])\n",
    "            print(\"Predicted :\",self.test_predicted_tags[i])\n",
    "        \n",
    "    def get_tag_metrics(self):\n",
    "        '''\n",
    "        Prints the per POS precision,recall and F1 score of predicted tags\n",
    "        '''\n",
    "        \n",
    "        print (\"{:<10} {:<10} {:<10} {:<10} {:<10}\".format('TAG', 'PRECISION', 'RECALL','F1_SCORE','ACCURACY'))\n",
    "        \n",
    "        for key in self.tag_metrics.keys():\n",
    "            precision = str(round(self.tag_metrics[key]['Precision'], 2))\n",
    "            recall    = str(round(self.tag_metrics[key]['Recall'], 2))\n",
    "            F1_score  = str(round(self.tag_metrics[key]['F1_score'], 2))\n",
    "            accuracy  = str(round(self.tag_metrics[key]['Accuracy'], 2))\n",
    "            print (\"{:<10} {:<10} {:<10} {:<10} {:<10}\".format(key, precision,recall,F1_score,accuracy)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Pre-processing \n",
    "'''\n",
    "def add_begin_end_tags(sentences):\n",
    "    \n",
    "    app_sent=[]\n",
    "    for sentence in sentences:\n",
    "      sentence.insert(0,('^','^')) # To find initial probabilities \n",
    "      sentence.append(('<EOS>','<EOS>')) # To make sure every sentence ends with a '.' to ease processing\n",
    "      app_sent.append(sentence)\n",
    "    return app_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Convert only non-noun words from the corpus to lower case\n",
    "'''\n",
    "def convert_to_lower(sentences):\n",
    "    app_sent=[]\n",
    "    for i in range(len(sentences)) :\n",
    "        for j in range(len(sentences[i])):\n",
    "            if(sentences[i][j][1] != 'NOUN'):\n",
    "                sentences[i][j] = (sentences[i][j][0].lower(),sentences[i][j][1])\n",
    "        app_sent.append(sentences[i])\n",
    "    return app_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Get all possible word tags for Viterbi Tagging\n",
    "'''\n",
    "def get_word_tag_mapping(sentences):\n",
    "    \n",
    "    word_tag_map ={}\n",
    "    for sentence in sentences:\n",
    "        for word,tag in sentence:\n",
    "            if word not in word_tag_map:\n",
    "                word_tag_map[word] = [tag]\n",
    "            if tag not in word_tag_map[word]:\n",
    "                word_tag_map[word].append(tag)\n",
    "    return word_tag_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Calculate average of scores obtained using 5-fold cross validation\n",
    "'''\n",
    "\n",
    "def avg(dict_list,folds):\n",
    "    avg_dict = defaultdict(lambda: defaultdict(lambda:0))\n",
    "    for d in dict_list:\n",
    "        for tag in d.keys():\n",
    "            avg_dict[tag]['Precision'] += d[tag]['Precision']/folds\n",
    "            avg_dict[tag]['Recall'] += d[tag]['Recall']/folds\n",
    "            avg_dict[tag]['F1_score'] += d[tag]['F1_score']/folds\n",
    "            avg_dict[tag]['Accuracy'] += d[tag]['Accuracy']/folds\n",
    "            \n",
    "    print (\"{:<10} {:<10} {:<10} {:<10} \".format('TAG', 'PRECISION', 'RECALL','F1_SCORE'))\n",
    "        \n",
    "    for key in avg_dict.keys():\n",
    "        precision = str(round(avg_dict[key]['Precision'], 2))\n",
    "        recall    = str(round(avg_dict[key]['Recall'], 2))\n",
    "        F1_score  = str(round(avg_dict[key]['F1_score'], 2))\n",
    "        accuracy  = str(round(avg_dict[key]['Accuracy'], 2))\n",
    "        print (\"{:<10} {:<10} {:<10} {:<10} \".format(key, precision,recall,F1_score)) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Taking sentence/tag pairs from brown corpus and doing cross validation\n",
    "''' \n",
    "sentences = brown.tagged_sents(tagset='universal')\n",
    "processed_sentences = add_begin_end_tags(sentences)\n",
    "processed_sentences = convert_to_lower(processed_sentences)\n",
    "tag_map = get_word_tag_mapping(processed_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TAG        PRECISION  RECALL     F1_SCORE   \n",
      "^          1.0        1.0        1.0        \n",
      "ADV        0.9        0.82       0.86       \n",
      ".          1.0        1.0        1.0        \n",
      "PRON       1.0        0.93       0.97       \n",
      "VERB       0.94       0.95       0.94       \n",
      "ADP        0.93       0.9        0.91       \n",
      "DET        0.99       0.98       0.99       \n",
      "NOUN       0.95       0.95       0.95       \n",
      "ADJ        0.92       0.91       0.91       \n",
      "<EOS>      1.0        1.0        1.0        \n",
      "CONJ       0.99       0.99       0.99       \n",
      "NUM        0.97       0.99       0.98       \n",
      "PRT        0.67       0.93       0.78       \n",
      "X          0.44       0.83       0.57       \n",
      "      |             <                                                                                     |\n",
      "      |             E                           C             N             P             V               |\n",
      "      |             O      A      A      A      O      D      O      N      R      P      E               |\n",
      "      |             S      D      D      D      N      E      U      U      O      R      R               |\n",
      "      |      .      >      J      P      V      J      T      N      M      N      T      B      X      ^ |\n",
      "------+---------------------------------------------------------------------------------------------------+\n",
      "    . |<147206>     .      .     20      .      .      .      1      .      .      .      .     20      . |\n",
      "<EOS> |      . <57340>     .      .      .      .      .      .      .      .      .      .      .      . |\n",
      "  ADJ |      .      . <75965>    60   3433      .      .   2372      3      1    370    737     20      . |\n",
      "  ADP |      .      .    143<129586>  4486      3   2186    116      .   1797    978     43     27      . |\n",
      "  ADV |      .      .   3407    733 <46176>   135     71    208      .      3    401     84      4      . |\n",
      " CONJ |      .      .      .    148     46 <37901>    61      1      .      .      .      .      4      . |\n",
      "  DET |      .      .      .      1    336    109<134589>     2      .    989      2      .     18      . |\n",
      " NOUN |      .      .   3698    138    675      .      .<262537>   217     15    199   8647    101      . |\n",
      "  NUM |      .      .      2      1      .      .      2    481 <14651>     .      .      .      5      . |\n",
      " PRON |      .      .      .      .      .      .     17     19      . <46066>     1      .      6      . |\n",
      "  PRT |      .      .     15  12955    960      .      1     33      .      7 <27764>     6      4      . |\n",
      " VERB |      .      .    430   1086     12      2      .   9723      .      .     38<173049>    37      . |\n",
      "    X |    359      .     61     38    115      1     92     65      3    456     76    184  <1140>     . |\n",
      "    ^ |      .      .      .      .      .      .      .      .      .      .      .      .      . <57340>|\n",
      "------+---------------------------------------------------------------------------------------------------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "OVERALL ACCURACY : 94.94 %\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Finding accuracies via 5 fold cross validation\n",
    "'''\n",
    "sent = np.array(processed_sentences,dtype=object)\n",
    "\n",
    "kfold = KFold(n_splits=FOLDS,shuffle=True)\n",
    "tag_metric_list = []\n",
    "predicted_tags_list = []\n",
    "actual_tags_list = []\n",
    "overall_accuracy = 0\n",
    "\n",
    "for train, test in kfold.split(sent):\n",
    "    train_data = list(sent[train])\n",
    "    test_data = list(sent[test])                 \n",
    "    hmm = HMM_POS(train_data,test_data,token_tags_map=tag_map)\n",
    "    hmm.calculate()\n",
    "    tag_metric_list.append(hmm.tag_metrics)\n",
    "    predicted_tags_list.extend(list(chain.from_iterable(hmm.test_predicted_tags)))\n",
    "    actual_tags_list.extend(list(chain.from_iterable(hmm.test_actual_tags)))\n",
    "    overall_accuracy += hmm.accuracy()/FOLDS\n",
    "    \n",
    "# Reporting precision,recall,F1 score  \n",
    "\n",
    "avg(tag_metric_list,FOLDS)\n",
    "\n",
    "# Printing final confusion matrix\n",
    "'''\n",
    "NOTE : This is the confusion matrix plotted over total of all predictions obtained from the FOLDS=5 folds\n",
    "Divide by FOLDS in case average is required\n",
    "'''\n",
    "\n",
    "print(ConfusionMatrix(predicted_tags_list,actual_tags_list))\n",
    "\n",
    "# Printing overall accuracy \n",
    "\n",
    "print(\"OVERALL ACCURACY :\",round(overall_accuracy,4)*100,\"%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
