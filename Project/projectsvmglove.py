# -*- coding: utf-8 -*-
"""ProjectSVMGlove.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1oP_lPuxcr0qYni8HvGSS-rCFlUCtfUYE
"""

import numpy as np
import pandas as pd

import matplotlib.pyplot as plt
import seaborn as sns

import warnings
import pickle

from google.colab import drive

drive.mount('/content/gdrive')

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/gdrive/My\ Drive/NLP_Project

with open(r"processed_english.pkl", "rb") as input_file:
    data = pickle.load(input_file)

import re
from pymongo import MongoClient
from collections import defaultdict
from nltk.corpus import wordnet as wn
#from transliteration import transliterate_word
import nltk
from nltk import word_tokenize
from nltk.util import ngrams as ngrams_creator
#from googletrans import Translator
from nltk.tokenize import word_tokenize
from string import punctuation 
from nltk.corpus import stopwords 
import pickle
#nltk.download('punkt')
nltk.download('vader_lexicon')
from nltk.corpus import sentiwordnet as swn
from nltk.sentiment.vader import SentimentIntensityAnalyzer

'''
Utility function to handle commonly used short forms
'''

def handle_short_forms(w):
    if w == 'h':
        return 'hai'
    elif w == 'n':
        return 'na'
    elif w == 'da':
        return 'the'
    elif w == 'wid':
        return 'with'
    elif w == 'pr':
        return 'par'
    elif w == 'mattt':
        return 'mat'
    elif w == 'vo':
        return 'woh'
    elif w == 'ki':
        return 'kee'
    elif w == 'ap':
        return 'aap'
    elif w == 'bs':
        return 'bas'
    elif w == 'goood':
        return 'very good'
    elif w == 'tera':
        return 'teraa'
    elif w == 'cnfsn':
        return 'confusion'
    elif w == 'ka':
        return 'kaa'
    elif w == 'rkhi':
        return 'rakhi'
    elif w == 'thts':
        return 'thats'
    elif w == 'cald':
        return 'called'
    elif w == 'tabhe':
        return 'tabhi'
    elif w == 'pta':
        return 'pata'
    elif w == 'b':
        return 'bhi'
    elif w == 'nai':
        return 'nahi'
    elif w == 'f':
        return 'of'
    elif w == 'd':
        return 'the'
    else:
        return w
    
'''
Translate word . 
'''
def translate(word):
    return translator.translate(word,src='hi' , dest='en').text

'''
Self defined contractions
''' 
def load_dict_contractions():
    
    return {
        "ain't":"is not",
        "amn't":"am not",
        "aren't":"are not",
        "can't":"cannot",
        "'cause":"because",
        "couldn't":"could not",
        "couldn't've":"could not have",
        "could've":"could have",
        "daren't":"dare not",
        "daresn't":"dare not",
        "dasn't":"dare not",
        "didn't":"did not",
        "doesn't":"does not",
        "don't":"do not",
        "e'er":"ever",
        "em":"them",
        "everyone's":"everyone is",
        "finna":"fixing to",
        "gimme":"give me",
        "gonna":"going to",
        "gon't":"go not",
        "gotta":"got to",
        "hadn't":"had not",
        "hasn't":"has not",
        "haven't":"have not",
        "he'd":"he would",
        "he'll":"he will",
        "he's":"he is",
        "he've":"he have",
        "how'd":"how would",
        "how'll":"how will",
        "how're":"how are",
        "how's":"how is",
        "I'd":"I would",
        "I'll":"I will",
        "i'll":"I will",
        "I'm":"I am",
        "I'm'a":"I am about to",
        "I'm'o":"I am going to",
        "isn't":"is not",
        "it'd":"it would",
        "it'll":"it will",
        "it's":"it is",
        "I've":"I have",
        "kinda":"kind of",
        "let's":"let us",
        "mayn't":"may not",
        "may've":"may have",
        "mightn't":"might not",
        "might've":"might have",
        "mustn't":"must not",
        "mustn't've":"must not have",
        "must've":"must have",
        "needn't":"need not",
        "ne'er":"never",
        "o'":"of",
        "o'er":"over",
        "ol'":"old",
        "oughtn't":"ought not",
        "shalln't":"shall not",
        "shan't":"shall not",
        "she'd":"she would",
        "she'll":"she will",
        "she's":"she is",
        "shouldn't":"should not",
        "shouldn't've":"should not have",
        "should've":"should have",
        "somebody's":"somebody is",
        "someone's":"someone is",
        "something's":"something is",
        "that'd":"that would",
        "that'll":"that will",
        "that're":"that are",
        "that's":"that is",
        "there'd":"there would",
        "there'll":"there will",
        "there're":"there are",
        "there's":"there is",
        "these're":"these are",
        "they'd":"they would",
        "they'll":"they will",
        "they're":"they are",
        "they've":"they have",
        "this's":"this is",
        "those're":"those are",
        "'tis":"it is",
        "'twas":"it was",
        "wanna":"want to",
        "wasn't":"was not",
        "we'd":"we would",
        "we'd've":"we would have",
        "we'll":"we will",
        "we're":"we are",
        "weren't":"were not",
        "we've":"we have",
        "what'd":"what did",
        "what'll":"what will",
        "what're":"what are",
        "what's":"what is",
        "what've":"what have",
        "when's":"when is",
        "where'd":"where did",
        "where're":"where are",
        "where's":"where is",
        "where've":"where have",
        "which's":"which is",
        "who'd":"who would",
        "who'd've":"who would have",
        "who'll":"who will",
        "who're":"who are",
        "who's":"who is",
        "who've":"who have",
        "why'd":"why did",
        "why're":"why are",
        "why's":"why is",
        "won't":"will not",
        "wouldn't":"would not",
        "would've":"would have",
        "y'all":"you all",
        "you'd":"you would",
        "you'll":"you will",
        "you're":"you are",
        "you've":"you have",
        "Whatcha":"What are you",
        "whatcha":"What are you",
        "luv":"love",
        "sux":"sucks"
        }

'''
Handling short forms and contractions in the sentences 
'''
long_form_dict = load_dict_contractions()
def expand_sent(sentence):
    final_sent =""
    res = " ".join(long_form_dict.get(ele, ele) for ele in sentence.split()) 
    for word in res.split():
        final_sent += (handle_short_forms(word))+ " "
    return final_sent

data.shape

l=[]
for sent in data.sentence_eng:
  l.append(expand_sent(sent))

print(l[2:5])

data['sentence_eng']=l

'''
Emoticon processing
'''
emoji_list = pd.read_csv('emoji.csv',sep=",")

def find_emoji(sentence):
    positive = 0
    negative = 0
    neutral  = 0
    sentiment =[]
    for word in sentence.split():
        if not emoji_list[emoji_list['Emoji']==word].empty:
            positive += emoji_list.iloc[emoji_list.index[emoji_list['Emoji'] == word].tolist()[0]]['Positive']
            negative += emoji_list.iloc[emoji_list.index[emoji_list['Emoji'] == word].tolist()[0]]['Negative']
            neutral += emoji_list.iloc[emoji_list.index[emoji_list['Emoji'] == word].tolist()[0]]['Neutral']
    return positive,negative,neutral

'''
Slang word processing
'''
slang_list = pd.read_csv('Hinglish_Profanity_List.csv',sep=",",header=None)
slang_list.columns=['hindi_word','meaning','rating']

def find_slang(sentence):
    slang_exists = False
    total_rating = 0
    for word in sentence.split():
        if not slang_list[slang_list['hindi_word']==word].empty:
            try:
                total_rating += slang_list.iloc[slang_list.index[slang_list['hindi_word'] == word].tolist()[0]]['rating']
            except:
                total_rating +=0
            slang_exists = True
    try:
        total_rating = int(total_rating)
    except:
        total_rating = 0
    return slang_exists,total_rating

data['slang_exists']= data['sentence_mixed'].apply(find_slang)

data[['slang_existance', 'slang_rating']] = pd.DataFrame(data['slang_exists'].tolist(), index=data.index) 
data =data.drop(columns=['slang_exists'])

data['emoji_sentiment'] = data['sentence_mixed'].apply(find_emoji)
data[['positive_emoji', 'negative_emoji','neutral_emoji']] = pd.DataFrame(data['emoji_sentiment'].tolist(), index=data.index) 
data =data.drop(columns=['emoji_sentiment'])

'''
Calculating the sentiment polarity score
'''

sid = SentimentIntensityAnalyzer()
neg_score=[]
pos_score=[]
neu_score=[]
for sent in data.sentence_eng:
  ss = sid.polarity_scores(sent)
  neg_score.append(ss['neg'])
  pos_score.append(ss['pos'])
  neu_score.append(ss['neu'])

data['neg_score']= neg_score
data['pos_score']= pos_score
data['neu_score']= neu_score

data.tail(10)

# removing unwanted patterns from the data

import re
import nltk

nltk.download('stopwords')
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer

#Splitting the data

train = data[:5000]
test = data[5000:]

'''
fetching the sentence/comments from the test dataset and cleaning it
'''

train_corpus = []

for i in range(0, 5000):
  review = re.sub('[^a-zA-Z]', ' ', train['sentence_eng'][i])
  review = review.lower()
  review = review.split()
  
  ps = PorterStemmer()
  
  # stemming
  review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]
  
  # joining them back with space
  review = ' '.join(review)
  train_corpus.append(review)

'''
fetching the sentence/comments from the test dataset and cleaning it
'''

test_corpus = []

for i in range(5000, 6137):
  review = re.sub('[^a-zA-Z]', ' ', test['sentence_eng'][i])
  review = review.lower()
  review = review.split()
  
  ps = PorterStemmer()
  
  # stemming
  review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]
  
  # joining them back with space
  review = ' '.join(review)
  test_corpus.append(review)

#Conversion of snetiment into numerical sentiment

train =train.replace("positive",1)
train =train.replace("negative",2)
train = train.replace("neutral",0)

train.head()

y = train.iloc[:, 2]
print(y)

from nltk.tokenize import WordPunctTokenizer
tokenizer = WordPunctTokenizer()

data_tok = [tokenizer.tokenize(d.lower()) for d in train_corpus]

import gensim.downloader as api
model = api.load('glove-twitter-50')

import numpy as np
def get_phrase_embedding(phrase):
    """
    Convert phrase to a vector by aggregating it's word embeddings. Just take an 
    average of vectors for all tokens in the phrase with some weights.
    """
    
    vector = np.zeros([model.vector_size], dtype='float32')
    
    # 1. lowercase phrase
    phrase = phrase.lower()
    
    # 2. tokenize phrase
    phrase_tokens = tokenizer.tokenize(phrase)
    
    # 3. average word vectors for all words in tokenized phrase, skip words that are not in model's vocabulary
    divisor = 0
    for word in phrase_tokens:
        if word in model.vocab:
            divisor += 1
            vector = vector + model.get_vector(word)
    
    if divisor != 0: vector /= divisor
    
    return vector

'''
Embeddings for Training Corpus
'''
vector_matrix_x_train = list(map(get_phrase_embedding, train_corpus))

'''
Embeddings for Training Corpus
'''
vector_matrix_x_test = list(map(get_phrase_embedding, test_corpus))

numerical_features_t = train[['neg_score','pos_score', 'neu_score', 'positive_emoji', 'negative_emoji', 'neutral_emoji', 'slang_rating']]
print(numerical_features_t.values.tolist())

'''
Stacking numerical features along with textual features
'''

combinedFeatures = np.hstack([numerical_features_t, np.array(vector_matrix_x_train)])

numerical_features_test = test[['neg_score','pos_score', 'neu_score', 'positive_emoji', 'negative_emoji', 'neutral_emoji', 'slang_rating']]
print(numerical_features_test.values.tolist())

combinedFeatures_test = np.hstack([numerical_features_test, np.array(vector_matrix_x_test)])

from sklearn.model_selection import train_test_split
'''
Train test validation split
'''
x_train, x_valid, y_train, y_valid = train_test_split(combinedFeatures, y, test_size = 0.25, random_state = 42)

print(x_train.shape)
print(x_valid.shape)
print(y_train.shape)
print(y_valid.shape)
print(combinedFeatures_test.shape)

# standardization

from sklearn.preprocessing import StandardScaler

sc = StandardScaler()

x_train = sc.fit_transform(x_train)
x_valid = sc.transform(x_valid)
x_test = sc.transform(combinedFeatures_test)

'''
Random Forest Classifer
'''


from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import confusion_matrix
from sklearn.metrics import f1_score

model = RandomForestClassifier()
model.fit(x_train, y_train)

y_pred = model.predict(x_valid)

print("Training Accuracy :", model.score(x_train, y_train))
print("Validation Accuracy :", model.score(x_valid, y_valid))

# calculating the f1 score for the validation set
print("F1 score :", f1_score(y_valid, y_pred,average='macro'))

# confusion matrix
cm = confusion_matrix(y_valid, y_pred)
print(cm)

'''
Logistic Regression Classifier
'''
from sklearn.linear_model import LogisticRegression

model = LogisticRegression()
model.fit(x_train, y_train)

y_pred = model.predict(x_valid)

print("Training Accuracy :", model.score(x_train, y_train))
print("Validation Accuracy :", model.score(x_valid, y_valid))

# calculating the f1 score for the validation set
print("f1 score :", f1_score(y_valid, y_pred,average='macro'))

# confusion matrix
cm = confusion_matrix(y_valid, y_pred)
print(cm)

'''
Decision Tree Classifier
'''

from sklearn.tree import DecisionTreeClassifier

model = DecisionTreeClassifier()
model.fit(x_train, y_train)

y_pred = model.predict(x_valid)

print("Training Accuracy :", model.score(x_train, y_train))
print("Validation Accuracy :", model.score(x_valid, y_valid))

# calculating the f1 score for the validation set
print("f1 score :", f1_score(y_valid, y_pred,average='macro'))

# confusion matrix
cm = confusion_matrix(y_valid, y_pred)
print(cm)

'''
SVM Classifier
'''

from sklearn.svm import SVC

model = SVC()
model.fit(x_train, y_train)

y_pred = model.predict(x_valid)

print("Training Accuracy :", model.score(x_train, y_train))
print("Validation Accuracy :", model.score(x_valid, y_valid))

# calculating the f1 score for the validation set
print("f1 score :", f1_score(y_valid, y_pred,average='macro'))

# confusion matrix
cm = confusion_matrix(y_valid, y_pred)
print(cm)

'''
xgBoost Classifier
'''

from xgboost import XGBClassifier

model = XGBClassifier()
model.fit(x_train, y_train)

y_pred = model.predict(x_valid)

print("Training Accuracy :", model.score(x_train, y_train))
print("Validation Accuracy :", model.score(x_valid, y_valid))

# calculating the f1 score for the validation set
print("f1 score :", f1_score(y_valid, y_pred,average='macro'))

# confusion matrix
cm = confusion_matrix(y_valid, y_pred)
print(cm)

pd.options.display.max_colwidth = 300
train.head(2)
t = train[1:]
t.head()
#print(test)
print(y_valid)
print(y_pred)

