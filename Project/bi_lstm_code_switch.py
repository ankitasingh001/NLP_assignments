# -*- coding: utf-8 -*-
"""Bi_LSTM_Code_switch.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gx_o54BQHJXGuhoqX9EoSgXVybk_u4TD
"""

import pickle
import nltk
from nltk.corpus import stopwords
stopwords = set(stopwords.words('english'))
nltk.download('wordnet')
nltk.download('stopwords')
from nltk.tokenize import TweetTokenizer
from nltk.corpus import wordnet as wn
tknzr = TweetTokenizer()
import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.python.keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional,Input,Activation,Flatten,concatenate
from tensorflow.python.keras.models import Model,Sequential,InputLayer
from tensorflow.python.keras.preprocessing.sequence import pad_sequences
from keras.preprocessing.text import Tokenizer
from sklearn.model_selection import train_test_split

'''
Utility function to get tokens from sentence
'''
def get_tokens(sentence):
    tokens = tknzr.tokenize(sentence)
    tokens = [token for token in tokens if (token not in stopwords and len(token) > 1)]
    tokens = [get_lemma(token) for token in tokens]
    return (tokens)

'''
Getting lemma of words
'''

def get_lemma(word):
    lemma = wn.morphy(word)
    if lemma is None:
        return word
    else:
        return lemma

'''
Generating word vectors from embeddings
Final embeddings used  =  glove twitter - 50 dimensional (Code crashes in higher ones)
'''
embedding_path = "glove.twitter.27B.50d.txt" 

def get_word2vec(file_path):
    file = open(embedding_path, "r")
    if (file):
        word2vec = dict()
        split = file.read().splitlines()
        for line in split:
            key = line.split(' ',1)[0] # the first word is the key
            value = np.array([float(val) for val in line.split(' ')[1:]])
            word2vec[key] = value
        return (word2vec)
    else:
        print("invalid file path")
w2v = get_word2vec(embedding_path)

'''
Importing processed file obtained after pre-processing
'''
with open("processedtillnow.pkl", "rb") as input_file:
    df = pickle.load(input_file)

# with open("processed_english.pkl", "rb") as input_file:
#     df = pickle.load(input_file)

token_list = (df['sentence_eng'].apply(get_tokens))

df

'''
Encoding labels/sentiments
'''
from sklearn import preprocessing
le = preprocessing.LabelEncoder()
Y_new = df['sentiment']
Y_new = le.fit_transform(Y_new)

slang_ex = df['slang_existance']
slang_ex = le.fit_transform(slang_ex)

new_features = df[['slang_rating', 'positive_emoji', 'negative_emoji','neutral_emoji']].copy()
new_features['slang_exists'] = slang_ex

'''
Tokenisation
'''
t = Tokenizer()
t.fit_on_texts(token_list)
vocab_size = len(t.word_index) + 1
'''
Integer encode the documents
'''
encoded_docs = t.texts_to_sequences(df['sentence_eng'])
'''
Add padding
'''
max_length = 100
X = pad_sequences(encoded_docs, maxlen=max_length, padding='post')
y = Y_new

'''
get the embedding matrix from the embedding layer
'''
from numpy import zeros
embedding_matrix = zeros((vocab_size, 50))
for word, i in t.word_index.items():
    embedding_vector = w2v.get(word)
    if embedding_vector is not None:
        embedding_matrix[i] = embedding_vector

'''
Split into test and training data
'''

X_train,X_test, Y_train, Y_test =  train_test_split(X, y,test_size =0.20,random_state= 4 )

'''
Build the bi-LSTM model
'''
max_len =100
inputx = Input(shape=(max_len,), name='nlp_input')
#meta_input = Input(shape=(5,), name='meta_input') # adding other emoticons/slang embeddings
model = Embedding(vocab_size,50,weights=[embedding_matrix],input_length=max_len)(inputx)
model =  Bidirectional (LSTM (50,return_sequences=True,dropout=0.50),merge_mode='concat')(model)
#model = concatenate([model, meta_input])(model)
model = TimeDistributed(Dense(50,activation='relu'))(model)
model = Flatten()(model)
model = Dense(50,activation='relu')(model)
output = Dense(3,activation='softmax')(model)
model = Model(inputx,output)
model.compile(loss='sparse_categorical_crossentropy',optimizer='adam', metrics=['accuracy'])

model.summary()

model.fit(X_train,Y_train,validation_split=0.25,epochs=10, verbose = 2)

'''
Calculate accuracy
'''
loss, accuracy = model.evaluate(X_test, Y_test, verbose=2)
print('Accuracy: %f' % (accuracy*100))

'''
Generate classification report
'''

from sklearn.metrics import classification_report,confusion_matrix
Y_pred = model.predict(X_test)
y_pred = np.array([np.argmax(pred) for pred in Y_pred])
print('  Classification Report:\n',classification_report(Y_test,y_pred),'\n')